{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wechat_big_data_challenge",
      "provenance": [],
      "authorship_tag": "ABX9TyNwmzRiuWob8/qEH3xqLrx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neowalter/we_big_data_challenge/blob/main/wechat_big_data_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KvQr12NdfAn",
        "outputId": "41f682c4-fd72-433b-ae28-956ec2596e77"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('http://dldir1.qq.com/weixin/wealgo/wechat_algo_data1.zip', 'data.zip')\n",
        "os.listdir()\n",
        "\n",
        "# or use !wget"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'data.zip',\n",
              " 'data',\n",
              " '.ipynb_checkpoints',\n",
              " 'wechat_algo_data1.zip',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyku6od9fhlg",
        "outputId": "92c0eb74-6e37-49b4-ff93-8538f6c15046"
      },
      "source": [
        "!unzip /content/data.zip -d /content/data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/data.zip\n",
            "   creating: /content/data/wechat_algo_data1/\n",
            "  inflating: /content/data/wechat_algo_data1/test_a.csv  \n",
            "  inflating: /content/data/wechat_algo_data1/feed_info.csv  \n",
            "  inflating: /content/data/wechat_algo_data1/feed_embeddings.csv  \n",
            "  inflating: /content/data/wechat_algo_data1/README.md  \n",
            "  inflating: /content/data/wechat_algo_data1/user_action.csv  \n",
            "  inflating: /content/data/wechat_algo_data1/submit_demo_初赛a.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ruM_3NVfug_",
        "outputId": "705769ab-00f8-4fd0-b3c1-cd77eb7962e4"
      },
      "source": [
        "!pip install deepctr==0.8.5"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepctr==0.8.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/23/a0c89b3a1631f8017dde94ee096db6ba14dfe0c996df8d5a0bdfb795ca54/deepctr-0.8.5-py3-none-any.whl (116kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 28.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 23.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 102kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 112kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deepctr==0.8.5) (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from deepctr==0.8.5) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deepctr==0.8.5) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deepctr==0.8.5) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr==0.8.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr==0.8.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr==0.8.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deepctr==0.8.5) (2.10)\n",
            "Installing collected packages: deepctr\n",
            "Successfully installed deepctr-0.8.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XZTKgZdf7SA",
        "outputId": "a015df15-4f7d-40ad-897a-b974dec1b700"
      },
      "source": [
        "!pip install deepctr==0.8.5 --no-deps"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepctr==0.8.5 in /usr/local/lib/python3.7/dist-packages (0.8.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJYZ3HcygIse"
      },
      "source": [
        "# MMOE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skz8sxhZgKlu"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from deepctr.feature_column import build_input_features, input_from_feature_columns\n",
        "from deepctr.layers.utils import combined_dnn_input\n",
        "from deepctr.layers.core import PredictionLayer, DNN\n",
        "\n",
        "from tensorflow.python.keras.initializers import glorot_normal\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "\n",
        "\n",
        "class MMOELayer(Layer):\n",
        "    \"\"\"\n",
        "    The Multi-gate Mixture-of-Experts layer in MMOE model\n",
        "      Input shape\n",
        "        - 2D tensor with shape: ``(batch_size,units)``.\n",
        "      Output shape\n",
        "        - A list with **num_tasks** elements, which is a 2D tensor with shape: ``(batch_size, output_dim)`` .\n",
        "      Arguments\n",
        "        - **num_tasks**: integer, the number of tasks, equal to the number of outputs.\n",
        "        - **num_experts**: integer, the number of experts.\n",
        "        - **output_dim**: integer, the dimension of each output of MMOELayer.\n",
        "    References\n",
        "      - [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_tasks, num_experts, output_dim, seed=1024, **kwargs):\n",
        "        self.num_experts = num_experts\n",
        "        self.num_tasks = num_tasks\n",
        "        self.output_dim = output_dim\n",
        "        self.seed = seed\n",
        "        super(MMOELayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = int(input_shape[-1])\n",
        "        self.expert_kernel = self.add_weight(\n",
        "            name='expert_kernel',\n",
        "            shape=(input_dim, self.num_experts * self.output_dim),\n",
        "            dtype=tf.float32,\n",
        "            initializer=glorot_normal(seed=self.seed))\n",
        "        self.gate_kernels = []\n",
        "        for i in range(self.num_tasks):\n",
        "            self.gate_kernels.append(self.add_weight(\n",
        "                name='gate_weight_'.format(i),\n",
        "                shape=(input_dim, self.num_experts),\n",
        "                dtype=tf.float32,\n",
        "                initializer=glorot_normal(seed=self.seed)))\n",
        "        super(MMOELayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        outputs = []\n",
        "        expert_out = tf.tensordot(inputs, self.expert_kernel, axes=(-1, 0))\n",
        "        expert_out = tf.reshape(expert_out, [-1, self.output_dim, self.num_experts])\n",
        "        for i in range(self.num_tasks):\n",
        "            gate_out = tf.tensordot(inputs, self.gate_kernels[i], axes=(-1, 0))\n",
        "            gate_out = tf.nn.softmax(gate_out)\n",
        "            gate_out = tf.tile(tf.expand_dims(gate_out, axis=1), [1, self.output_dim, 1])\n",
        "            output = tf.reduce_sum(tf.multiply(expert_out, gate_out), axis=2)\n",
        "            outputs.append(output)\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = {'num_tasks': self.num_tasks,\n",
        "                  'num_experts': self.num_experts,\n",
        "                  'output_dim': self.output_dim}\n",
        "        base_config = super(MMOELayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [input_shape[0], self.output_dim] * self.num_tasks\n",
        "\n",
        "\n",
        "def MMOE(dnn_feature_columns, num_tasks, tasks, num_experts=4, expert_dim=8, dnn_hidden_units=(128, 128),\n",
        "         l2_reg_embedding=1e-5, l2_reg_dnn=0, task_dnn_units=None, seed=1024, dnn_dropout=0, dnn_activation='relu'):\n",
        "    \"\"\"Instantiates the Multi-gate Mixture-of-Experts architecture.\n",
        "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
        "    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n",
        "    :param tasks: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n",
        "    :param num_experts: integer, number of experts.\n",
        "    :param expert_dim: integer, the hidden units of each expert.\n",
        "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of shared-bottom DNN\n",
        "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
        "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
        "    :param task_dnn_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN\n",
        "    :param seed: integer ,to use as random seed.\n",
        "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
        "    :param dnn_activation: Activation function to use in DNN\n",
        "    :return: a Keras model instance\n",
        "    \"\"\"\n",
        "    if num_tasks <= 1:\n",
        "        raise ValueError(\"num_tasks must be greater than 1\")\n",
        "    if len(tasks) != num_tasks:\n",
        "        raise ValueError(\"num_tasks must be equal to the length of tasks\")\n",
        "    for task in tasks:\n",
        "        if task not in ['binary', 'regression']:\n",
        "            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task))\n",
        "\n",
        "    features = build_input_features(dnn_feature_columns)\n",
        "\n",
        "    inputs_list = list(features.values())\n",
        "\n",
        "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
        "                                                                         l2_reg_embedding, seed)\n",
        "    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
        "    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
        "                  False, seed=seed)(dnn_input)\n",
        "    mmoe_outs = MMOELayer(num_tasks, num_experts, expert_dim)(dnn_out)\n",
        "    if task_dnn_units != None:\n",
        "        mmoe_outs = [DNN(task_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed)(mmoe_out) for mmoe_out in\n",
        "                     mmoe_outs]\n",
        "\n",
        "    task_outputs = []\n",
        "    for mmoe_out, task in zip(mmoe_outs, tasks):\n",
        "        logit = tf.keras.layers.Dense(\n",
        "            1, use_bias=False, activation=None)(mmoe_out)\n",
        "        output = PredictionLayer(task)(logit)\n",
        "        task_outputs.append(output)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=inputs_list,\n",
        "                                  outputs=task_outputs)\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUM5upfkhCOR"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__17jVSgXKh"
      },
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "def uAUC(labels, preds, user_id_list):\n",
        "    \"\"\"Calculate user AUC\"\"\"\n",
        "    user_pred = defaultdict(lambda: [])\n",
        "    user_truth = defaultdict(lambda: [])\n",
        "    for idx, truth in enumerate(labels):\n",
        "        user_id = user_id_list[idx]\n",
        "        pred = preds[idx]\n",
        "        truth = labels[idx]\n",
        "        user_pred[user_id].append(pred)\n",
        "        user_truth[user_id].append(truth)\n",
        "\n",
        "    user_flag = defaultdict(lambda: False)\n",
        "    for user_id in set(user_id_list):\n",
        "        truths = user_truth[user_id]\n",
        "        flag = False\n",
        "        # 若全是正样本或全是负样本，则flag为False\n",
        "        for i in range(len(truths) - 1):\n",
        "            if truths[i] != truths[i + 1]:\n",
        "                flag = True\n",
        "                break\n",
        "        user_flag[user_id] = flag\n",
        "\n",
        "    total_auc = 0.0\n",
        "    size = 0.0\n",
        "    for user_id in user_flag:\n",
        "        if user_flag[user_id]:\n",
        "            auc = roc_auc_score(np.asarray(user_truth[user_id]), np.asarray(user_pred[user_id]))\n",
        "            total_auc += auc \n",
        "            size += 1.0\n",
        "    user_auc = float(total_auc)/size\n",
        "    return user_auc\n",
        "\n",
        "\n",
        "def compute_weighted_score(score_dict, weight_dict):\n",
        "    '''基于多个行为的uAUC值，计算加权uAUC\n",
        "    Input:\n",
        "        scores_dict: 多个行为的uAUC值映射字典, dict\n",
        "        weights_dict: 多个行为的权重映射字典, dict\n",
        "    Output:\n",
        "        score: 加权uAUC值, float\n",
        "    '''\n",
        "    score = 0.0\n",
        "    weight_sum = 0.0\n",
        "    for action in score_dict:\n",
        "        weight = float(weight_dict[action])\n",
        "        score += weight*score_dict[action]\n",
        "        weight_sum += weight\n",
        "    score /= float(weight_sum)\n",
        "    score = round(score, 6)\n",
        "    return score\n",
        "\n",
        "\n",
        "def evaluate_deepctr(val_labels,val_pred_ans,userid_list,target):\n",
        "    eval_dict = {}\n",
        "    for i, action in enumerate(target):\n",
        "        eval_dict[action] = uAUC(val_labels[i], val_pred_ans[i], userid_list)\n",
        "    print(eval_dict)\n",
        "    weight_dict = {\"read_comment\": 4, \"like\": 3, \"click_avatar\": 2, \"favorite\": 1, \"forward\": 1,\n",
        "                   \"comment\": 1, \"follow\": 1}\n",
        "    weight_auc = compute_weighted_score(eval_dict, weight_dict)\n",
        "    print(\"Weighted uAUC: \", weight_auc)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03WMJu1KhMWY"
      },
      "source": [
        "# Run_mmoe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNbHCMxFheag"
      },
      "source": [
        "# import pandas as pd\n",
        "# data = pd.read_csv('./data/wechat_algo_data1/user_action.csv')\n",
        "# print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcmxcjQjlSPq",
        "outputId": "c1f6becb-0b9f-438d-cec7-0fd4380fbb64"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYRI-QwHhHdw",
        "outputId": "6d741b70-40ec-492c-f51a-17c7e32b5f77"
      },
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "\n",
        "from time import time\n",
        "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
        "\n",
        "# from mmoe import MMOE\n",
        "# from evaluation import evaluate_deepctr\n",
        "\n",
        "# GPU相关设置\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "# 设置GPU按需增长\n",
        "\n",
        "# config = tf.compat.v1.ConfigProto()\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    epochs = 2\n",
        "    batch_size = 512\n",
        "    embedding_dim = 16\n",
        "    target = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
        "    sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
        "    dense_features = ['videoplayseconds', ]\n",
        "\n",
        "    data = pd.read_csv('./data/wechat_algo_data1/user_action.csv')\n",
        "\n",
        "    feed = pd.read_csv('./data/wechat_algo_data1/feed_info.csv')\n",
        "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
        "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
        "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
        "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
        "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
        "    data = data.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
        "                      on='feedid')\n",
        "\n",
        "    test = pd.read_csv('./data/wechat_algo_data1/test_a.csv')\n",
        "    test = test.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
        "                      on='feedid')\n",
        "\n",
        "    # 1.fill nan dense_feature and do simple Transformation for dense features\n",
        "    data[dense_features] = data[dense_features].fillna(0, )\n",
        "    test[dense_features] = test[dense_features].fillna(0, )\n",
        "\n",
        "    data[dense_features] = np.log(data[dense_features] + 1.0)\n",
        "    test[dense_features] = np.log(test[dense_features] + 1.0)\n",
        "\n",
        "    print('data.shape', data.shape)\n",
        "    print('data.columns', data.columns.tolist())\n",
        "    print('unique date_: ', data['date_'].unique())\n",
        "\n",
        "    train = data[data['date_'] < 14]\n",
        "    val = data[data['date_'] == 14]  # 第14天样本作为验证集\n",
        "\n",
        "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
        "    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=embedding_dim)\n",
        "                              for feat in sparse_features] + [DenseFeat(feat, 1) for feat in dense_features]\n",
        "\n",
        "    dnn_feature_columns = fixlen_feature_columns\n",
        "    feature_names = get_feature_names(dnn_feature_columns)\n",
        "\n",
        "    # 3.generate input data for model\n",
        "    train_model_input = {name: train[name] for name in feature_names}\n",
        "    val_model_input = {name: val[name] for name in feature_names}\n",
        "    userid_list = val['userid'].astype(str).tolist()\n",
        "    test_model_input = {name: test[name] for name in feature_names}\n",
        "\n",
        "    train_labels = [train[y].values for y in target]\n",
        "    val_labels = [val[y].values for y in target]\n",
        "\n",
        "    # 4.Define Model,train,predict and evaluate\n",
        "    train_model = MMOE(dnn_feature_columns, num_tasks=4, expert_dim=8, dnn_hidden_units=(128, 128),\n",
        "                       tasks=['binary', 'binary', 'binary', 'binary'])\n",
        "    train_model.compile(\"adagrad\", loss='binary_crossentropy')\n",
        "    # print(train_model.summary())\n",
        "    for epoch in range(epochs):\n",
        "        history = train_model.fit(train_model_input, train_labels,\n",
        "                                  batch_size=batch_size, epochs=1, verbose=1)\n",
        "\n",
        "        val_pred_ans = train_model.predict(val_model_input, batch_size=batch_size * 4)\n",
        "        evaluate_deepctr(val_labels, val_pred_ans, userid_list, target)\n",
        "\n",
        "    t1 = time()\n",
        "    pred_ans = train_model.predict(test_model_input, batch_size=batch_size * 20)\n",
        "    t2 = time()\n",
        "    print('4个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
        "    ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
        "    print('4个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
        "\n",
        "    # 5.生成提交文件\n",
        "    for i, action in enumerate(target):\n",
        "        test[action] = pred_ans[i]\n",
        "    test[['userid', 'feedid'] + target].to_csv('result.csv', index=None, float_format='%.6f')\n",
        "    print('to_csv ok')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data.shape (7317882, 17)\n",
            "data.columns ['userid', 'feedid', 'date_', 'device', 'read_comment', 'comment', 'like', 'play', 'stay', 'click_avatar', 'forward', 'follow', 'favorite', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']\n",
            "unique date_:  [ 1  2  3  5  6  7  8 10 11 12 13 14  4  9]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/initializers/initializers_v1.py:48: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Train on 6708846 samples\n",
            "6708846/6708846 [==============================] - 125s 19us/sample - loss: 0.4016 - prediction_layer_loss: 0.1584 - prediction_layer_1_loss: 0.1386 - prediction_layer_2_loss: 0.0689 - prediction_layer_3_loss: 0.0356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6708846/6708846 [==============================] - 122s 18us/sample - loss: 0.3551 - prediction_layer_loss: 0.1527 - prediction_layer_1_loss: 0.1237 - prediction_layer_2_loss: 0.0507 - prediction_layer_3_loss: 0.0280\n",
            "{'read_comment': 0.5019326601105215, 'like': 0.48229420881983714, 'click_avatar': 0.3959533775663518, 'forward': 0.47446905348181834}\n",
            "Weighted uAUC:  0.472099\n",
            "4个目标行为421985条样本预测耗时（毫秒）：306.282\n",
            "4个目标行为2000条样本平均预测耗时（毫秒）：1.452\n",
            "to_csv ok\n",
            "CPU times: user 5min 37s, sys: 29.1 s, total: 6min 6s\n",
            "Wall time: 4min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XMwh6VrnFcK",
        "outputId": "2085e39f-f2aa-4dd1-a82d-2b4084b404f5"
      },
      "source": [
        "df = pd.read_csv('/content/result.csv')\n",
        "print(df)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        userid  feedid  read_comment      like  click_avatar   forward\n",
            "0        14298   67227      0.041320  0.035955      0.011964  0.005495\n",
            "1        68356   91864      0.040186  0.033981      0.011024  0.004991\n",
            "2        49925  104657      0.033912  0.023095      0.006318  0.002596\n",
            "3        60529   23738      0.046176  0.044510      0.016264  0.007938\n",
            "4       131482   69038      0.034401  0.023934      0.006650  0.002757\n",
            "...        ...     ...           ...       ...           ...       ...\n",
            "421980  133812   56450      0.050389  0.051844      0.020318  0.010320\n",
            "421981  231669   76501      0.059943  0.068163      0.030439  0.016591\n",
            "421982  179168   70550      0.026312  0.010773      0.002137  0.000730\n",
            "421983   92546   49432      0.044679  0.041967      0.014906  0.007172\n",
            "421984  205434    2109      0.044818  0.042160      0.014999  0.007229\n",
            "\n",
            "[421985 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpvuP1e3nMKK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}